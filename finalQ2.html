<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Question 2</title>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
        <link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        
        <script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
    </head>
    <body class="vscode-light">
        <h4 id="question-2">Question 2</h4>
<pre><code class="language-R"><div><span class="hljs-keyword">library</span>(readr) 
<span class="hljs-keyword">library</span>(car)
<span class="hljs-comment"># Read data from csv</span>
data &lt;- read_csv(<span class="hljs-string">"TableB3.csv"</span>)

data

<span class="hljs-comment"># a scatterplot matrix which include all the variables in the data set.</span>
<span class="hljs-comment"># Plot matrix of all variables.</span>
plot(data, col=<span class="hljs-string">"navy"</span>, main=<span class="hljs-string">"Matrix Scatterplot"</span>)
</div></code></pre>
<p><strong>Understanding Data:</strong></p>
<p>Inorder to get a high level overview of the data, i used the matrix scatter plot as shown below.</p>
<p align="left">
  <img src="file:////Users/mbpro/College/STAT/LRA/finalQ2_scatterplot.png" width="450" title="hover text">
</p>
<p>The above matrix plot helps to see the relationship between two columns and pattern in the datasets. For example, x1, x2, x3, x8, x9 and x10 seem to have similiar relationships. From the plot, y seems to have the same pattern with x1, x2, x3, x8, x9 and x10. Also, y seems to follow similar pattern with x6, x7 and x11.</p>
<p>In order to find the best model for predicting the y using different predictors in data, I look upon residuals error, p-value for the performance of the model.</p>
<p><strong>Fitting Models</strong></p>
<pre><code class="language-R"><div><span class="hljs-comment"># Fit data to linear model</span>
fit1 &lt;- lm(y ~ ., data = data)

<span class="hljs-comment"># Print the model summary</span>
summary(fit1)
<span class="hljs-comment"># Output</span>
<span class="hljs-comment"># Residuals:</span>
<span class="hljs-comment">#     Min      1Q  Median      3Q     Max </span>
<span class="hljs-comment"># -5.3441 -1.6711 -0.4486  1.4906  5.2508 </span>

<span class="hljs-comment"># Coefficients:</span>
<span class="hljs-comment">#              Estimate Std. Error t value Pr(&gt;|t|)  </span>
<span class="hljs-comment"># (Intercept) 17.339838  30.355375   0.571   0.5749  </span>
<span class="hljs-comment"># x1          -0.075588   0.056347  -1.341   0.1964  </span>
<span class="hljs-comment"># x2          -0.069163   0.087791  -0.788   0.4411  </span>
<span class="hljs-comment"># x3           0.115117   0.088113   1.306   0.2078  </span>
<span class="hljs-comment"># x4           1.494737   3.101464   0.482   0.6357  </span>
<span class="hljs-comment"># x5           5.843495   3.148438   1.856   0.0799 .</span>
<span class="hljs-comment"># x6           0.317583   1.288967   0.246   0.8082  </span>
<span class="hljs-comment"># x7          -3.205390   3.109185  -1.031   0.3162  </span>
<span class="hljs-comment"># x8           0.180811   0.130301   1.388   0.1822  </span>
<span class="hljs-comment"># x9          -0.397945   0.323456  -1.230   0.2344  </span>
<span class="hljs-comment"># x10         -0.005115   0.005896  -0.868   0.3971  </span>
<span class="hljs-comment"># x11          0.638483   3.021680   0.211   0.8350  </span>
<span class="hljs-comment"># ---</span>
<span class="hljs-comment"># Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span>

<span class="hljs-comment"># Residual standard error: 3.227 on 18 degrees of freedom</span>
<span class="hljs-comment">#   (2 observations deleted due to missingness)</span>
<span class="hljs-comment"># Multiple R-squared:  0.8355,    Adjusted R-squared:  0.7349 </span>
<span class="hljs-comment"># F-statistic:  8.31 on 11 and 18 DF,  p-value: 5.231e-05</span>
</div></code></pre>
<p>From the summary of fit1, I have the results of the model. From the model output I found that:</p>
<ul>
<li>
<p>The parameters - x2, x4, x6, x10 and x11 have t-value close to 0 and high pvalues. It shows that there is no significant relation with y.</p>
</li>
<li>
<p>The RSE is 3.227 where p-value is very small.</p>
</li>
</ul>
<p>The matrix scatterplot above shows that there is the high correlation between x1, x2, x3 and x10. When there are two or more variables strongly correlated it is called collinearity. I validate collinearity by using correlation matrix and VIF.</p>
<pre><code class="language-R"><div><span class="hljs-comment"># matrix of correlations between the variables</span>
cor(data, use = <span class="hljs-string">"complete.obs"</span>)
<span class="hljs-comment"># Output</span>
<span class="hljs-comment">#              y         x1         x2         x3          x4         x5</span>
<span class="hljs-comment"># y    1.0000000 -0.8721701 -0.7968304 -0.8495915  0.42237247  0.6347500</span>
<span class="hljs-comment"># x1  -0.8721701  1.0000000  0.9408473  0.9891628 -0.34697246 -0.6720903</span>
<span class="hljs-comment"># x2  -0.7968304  0.9408473  1.0000000  0.9643592 -0.28989951 -0.5509642</span>
<span class="hljs-comment"># x3  -0.8495915  0.9891628  0.9643592  1.0000000 -0.32599915 -0.6728661</span>
<span class="hljs-comment"># x4   0.4223725 -0.3469725 -0.2898995 -0.3259992  1.00000000  0.4137808</span>
<span class="hljs-comment"># x5   0.6347500 -0.6720903 -0.5509642 -0.6728661  0.41378081  1.0000000</span>
<span class="hljs-comment"># x6  -0.4718055  0.6427984  0.7614190  0.6531263  0.03748643 -0.2195283</span>
<span class="hljs-comment"># x7   0.7077682 -0.7719151 -0.6259445 -0.7461800  0.55823570  0.8717662</span>
<span class="hljs-comment"># x8  -0.7528208  0.8623681  0.8027387  0.8641224 -0.30415026 -0.5613315</span>
<span class="hljs-comment"># x9  -0.7629952  0.7974811  0.7105117  0.7881284 -0.37817358 -0.4534470</span>
<span class="hljs-comment"># x10 -0.8528801  0.9515520  0.8878810  0.9434871 -0.35845879 -0.5798617</span>
<span class="hljs-comment"># x11 -0.7212809  0.8244446  0.7086735  0.8012765 -0.44054570 -0.7546650</span>
<span class="hljs-comment">#              x6         x7         x8         x9        x10        x11</span>
<span class="hljs-comment"># y   -0.47180548  0.7077682 -0.7528208 -0.7629952 -0.8528801 -0.7212809</span>
<span class="hljs-comment"># x1   0.64279836 -0.7719151  0.8623681  0.7974811  0.9515520  0.8244446</span>
<span class="hljs-comment"># x2   0.76141897 -0.6259445  0.8027387  0.7105117  0.8878810  0.7086735</span>
<span class="hljs-comment"># x3   0.65312630 -0.7461800  0.8641224  0.7881284  0.9434871  0.8012765</span>
<span class="hljs-comment"># x4   0.03748643  0.5582357 -0.3041503 -0.3781736 -0.3584588 -0.4405457</span>
<span class="hljs-comment"># x5  -0.21952829  0.8717662 -0.5613315 -0.4534470 -0.5798617 -0.7546650</span>
<span class="hljs-comment"># x6   1.00000000 -0.2756386  0.4220680  0.3003862  0.5203669  0.3954893</span>
<span class="hljs-comment"># x7  -0.27563863  1.0000000 -0.6552065 -0.6551300 -0.7058126 -0.8506963</span>
<span class="hljs-comment"># x8   0.42206800 -0.6552065  1.0000000  0.8831512  0.9554541  0.6824919</span>
<span class="hljs-comment"># x9   0.30038618 -0.6551300  0.8831512  1.0000000  0.8994711  0.6326677</span>
<span class="hljs-comment"># x10  0.52036693 -0.7058126  0.9554541  0.8994711  1.0000000  0.7530353</span>
<span class="hljs-comment"># x11  0.39548928 -0.8506963  0.6824919  0.6326677  0.7530353  1.0000000</span>

car::vif(fit1)
<span class="hljs-comment">#Output</span>
<span class="hljs-comment">#         x1         x2         x3         x4         x5         x6         x7 </span>
<span class="hljs-comment"># 119.487804  42.800811 149.234409   2.060036   7.729187   5.324730  11.761341 </span>
<span class="hljs-comment">#         x8         x9        x10        x11 </span>
<span class="hljs-comment">#  20.917632   9.397108  85.744344   5.145052 </span>
</div></code></pre>
<p>From the correlation, I can see that there is a strong relation between x1, x2, x3, x8, x9 and x10. The y is also strongly correlated to x1, x2, x3, x8, x9 and x10. This situation is called collinearity. The problem of collinearity in the response is that it is difficult to find the individual effect on response. We should drop use only one of the collinear variables.</p>
<p>Out of x1, x2, x3, x8, x9 and x10, the output of the first model shows that x8 and y have a highly significant relation. I use x1 out of the other collinear variables in the next model.</p>
<p>Also, from fit1, I see x4, x6 and x11 have no significance to model.</p>
<pre><code class="language-R"><div>
<span class="hljs-comment"># Fit data to linear model</span>
fit2 &lt;- lm(y ~ x1 + x5 + x7, data = data)

<span class="hljs-comment"># Print the fit2 summary</span>
summary(fit2)

<span class="hljs-comment"># Output</span>
<span class="hljs-comment"># Residuals:</span>
<span class="hljs-comment">#     Min      1Q  Median      3Q     Max </span>
<span class="hljs-comment"># -6.7887 -1.9555  0.2436  1.6370  6.8531 </span>

<span class="hljs-comment"># Coefficients:</span>
<span class="hljs-comment">#              Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="hljs-comment"># (Intercept) 29.492027   6.655192   4.431 0.000131 ***</span>
<span class="hljs-comment"># x1          -0.043652   0.007756  -5.628    5e-06 ***</span>
<span class="hljs-comment"># x5           0.347945   2.057862   0.169 0.866949    </span>
<span class="hljs-comment"># x7           0.631227   2.006266   0.315 0.755377    </span>
<span class="hljs-comment"># ---</span>
<span class="hljs-comment"># Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span>

<span class="hljs-comment"># Residual standard error: 3.149 on 28 degrees of freedom</span>
<span class="hljs-comment"># Multiple R-squared:  0.7757,    Adjusted R-squared:  0.7517 </span>
<span class="hljs-comment"># F-statistic: 32.28 on 3 and 28 DF,  p-value: 3.145e-09</span>

car::vif(fit2)
<span class="hljs-comment"># Output</span>
<span class="hljs-comment">#       x1       x5       x7 </span>
<span class="hljs-comment"># 2.585583 3.475845 5.366865 </span>
</div></code></pre>
<p>The VIF output show s that I have almost got rid of collinearity problem (execpt for x7 has VIF &gt; 5).
In this model, I find x1 predictor p-value is highly significant. After excluding the collinear variable the F- statistic improved from 8.31 to 32.28 which is a good sign. But there is no improvement on RSE and adjusted R squared value. Let’s plot the residuals:</p>
<p align="left">
  <img src="file:////Users/mbpro/College/STAT/LRA/finalQ2_fit2_res.png" width="450" title="hover text">
</p>
<ul>
<li><strong>Residuals vs Fitted:</strong>
The plot of residuals versus fitted values indicates linearity in the data. A horizontal line, without distinct patterns is an indication for a linear relationship. And for this model it seems to have a pattern fitted with the residuals and fitted values. And it indicates a non-linear relationship in the data.</li>
</ul>
<p>This plot also shows some of the outliers lying far away from the middle of the graph.</p>
<ul>
<li>
<p><strong>Normal Q-Q:</strong>
This plot is used to examine whether the residuals are normally distributed. It’s good if residuals points follow the straight dashed line. And this model has its residuals normally distributed except for the tail data (the outliers).</p>
</li>
<li>
<p><strong>Scale-Location (or Spread-Location):</strong> This plot is used to check the homogeneity of variance of the residuals (homoscedasticity). Horizontal line with equally spread points is a good indication of homoscedasticity. This is not the case in this model and hence the model has a heteroscedasticity problem.</p>
</li>
<li>
<p><strong>Standardized Residuals vs Leverage:</strong> This plot is used to identify influential cases (outliers), that is extreme values that might influence the regression results when included or excluded from the analysis. This plot of standardized residuals versus leverage indicates the presence of a few outliers (example: point 12 and point 27)</p>
</li>
</ul>
<p>I try log transformation on both predictors and the response value and see how the performance of model changes. In the next model, I use the natural log to the y using log() and see the change in performance of the model.</p>
<pre><code class="language-R"><div>fit3 &lt;- lm(log(y) ~ x1 + x5 + x7, data = data)
summary(fit3)
<span class="hljs-comment"># Output</span>
<span class="hljs-comment"># Residuals:</span>
<span class="hljs-comment">#       Min        1Q    Median        3Q       Max </span>
<span class="hljs-comment"># -0.279565 -0.105526  0.009649  0.084624  0.237414 </span>

<span class="hljs-comment"># Coefficients:</span>
<span class="hljs-comment">#               Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="hljs-comment"># (Intercept)  3.6371922  0.2889610  12.587 4.79e-13 ***</span>
<span class="hljs-comment"># x1          -0.0021787  0.0003368  -6.469 5.22e-07 ***</span>
<span class="hljs-comment"># x5          -0.0786753  0.0893500  -0.881    0.386    </span>
<span class="hljs-comment"># x7           0.0563762  0.0871098   0.647    0.523    </span>
<span class="hljs-comment"># ---</span>
<span class="hljs-comment"># Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span>

<span class="hljs-comment"># Residual standard error: 0.1367 on 28 degrees of freedom</span>
<span class="hljs-comment"># Multiple R-squared:  0.7997,    Adjusted R-squared:  0.7783 </span>
<span class="hljs-comment"># F-statistic: 37.27 on 3 and 28 DF,  p-value: 6.522e-10</span>
plot(fit3)
</div></code></pre>
<p>The performance of the model is unaffected. The Adjusted R-squared  and F-statistic has not increased.</p>
<pre><code class="language-R"><div>
fit4 &lt;- lm((y) ~ log(x1) + log(x5) + log(x7), data = data)
summary(fit4)
<span class="hljs-comment"># Output</span>
<span class="hljs-comment"># Residuals:</span>
<span class="hljs-comment">#     Min      1Q  Median      3Q     Max </span>
<span class="hljs-comment"># -6.6366 -1.8211  0.2634  1.6410  4.4030 </span>

<span class="hljs-comment"># Coefficients:</span>
<span class="hljs-comment">#             Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="hljs-comment"># (Intercept)  109.360     14.965   7.308 5.89e-08 ***</span>
<span class="hljs-comment"># log(x1)      -13.898      1.690  -8.224 5.97e-09 ***</span>
<span class="hljs-comment"># log(x5)       -5.495      4.738  -1.160    0.256    </span>
<span class="hljs-comment"># log(x7)       -5.062      6.160  -0.822    0.418    </span>
<span class="hljs-comment"># ---</span>
<span class="hljs-comment"># Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span>

<span class="hljs-comment"># Residual standard error: 2.411 on 28 degrees of freedom</span>
<span class="hljs-comment"># Multiple R-squared:  0.8685,    Adjusted R-squared:  0.8544 </span>
<span class="hljs-comment"># F-statistic: 61.62 on 3 and 28 DF,  p-value: 1.885e-12</span>
plot(fit4, which =<span class="hljs-number">1</span>)
</div></code></pre>
<p>Here the performance of the model has increased. The Adjusted R-squared raised to 0.8544 and F-statistic is increased to 61.62.</p>
<p>Now, build regression model from a set of candidate predictor variables by entering and removing predictors based on p values, in a stepwise manner until there is no variable left to enter or remove any more.</p>
<pre><code class="language-R"><div>ols_step_both_p(fit4)
<span class="hljs-comment"># Output</span>
<span class="hljs-comment"># Stepwise Selection Method   </span>
<span class="hljs-comment"># ---------------------------</span>

<span class="hljs-comment"># Candidate Terms: </span>

<span class="hljs-comment"># 1. log(x1) </span>
<span class="hljs-comment"># 2. log(x5) </span>

<span class="hljs-comment"># We are selecting variables based on p value...</span>

<span class="hljs-comment"># Variables Entered/Removed: </span>

<span class="hljs-comment"># ✔ log(x1) </span>
<span class="hljs-comment"># ✔ log(x5) </span>


<span class="hljs-comment"># Final Model Output </span>
<span class="hljs-comment"># ------------------</span>

<span class="hljs-comment">#                         Model Summary                          </span>
<span class="hljs-comment"># --------------------------------------------------------------</span>
<span class="hljs-comment"># R                       0.930       RMSE                2.398 </span>
<span class="hljs-comment"># R-Squared               0.865       Coef. Var          11.856 </span>
<span class="hljs-comment"># Adj. R-Squared          0.856       MSE                 5.749 </span>
<span class="hljs-comment"># Pred R-Squared          0.834       MAE                 1.802 </span>
<span class="hljs-comment"># --------------------------------------------------------------</span>
<span class="hljs-comment">#  RMSE: Root Mean Square Error </span>
<span class="hljs-comment">#  MSE: Mean Square Error </span>
<span class="hljs-comment">#  MAE: Mean Absolute Error </span>

<span class="hljs-comment">#                                ANOVA                                 </span>
<span class="hljs-comment"># --------------------------------------------------------------------</span>
<span class="hljs-comment">#                 Sum of                                              </span>
<span class="hljs-comment">#                Squares        DF    Mean Square      F         Sig. </span>
<span class="hljs-comment"># --------------------------------------------------------------------</span>
<span class="hljs-comment"># Regression    1070.829         2        535.415    93.135    0.0000 </span>
<span class="hljs-comment"># Residual       166.715        29          5.749                     </span>
<span class="hljs-comment"># Total         1237.544        31                                    </span>
<span class="hljs-comment"># --------------------------------------------------------------------</span>

<span class="hljs-comment">#                                     Parameter Estimates                                      </span>
<span class="hljs-comment"># --------------------------------------------------------------------------------------------</span>
<span class="hljs-comment">#       model       Beta    Std. Error    Std. Beta       t        Sig       lower      upper </span>
<span class="hljs-comment"># --------------------------------------------------------------------------------------------</span>
<span class="hljs-comment"># (Intercept)    100.293        10.052                   9.977    0.000     79.734    120.851 </span>
<span class="hljs-comment">#     log(x1)    -12.910         1.181       -1.056    -10.935    0.000    -15.325    -10.496 </span>
<span class="hljs-comment">#     log(x5)     -7.707         3.877       -0.192     -1.988    0.056    -15.636      0.223 </span>
<span class="hljs-comment"># --------------------------------------------------------------------------------------------</span>

<span class="hljs-comment">#                              Stepwise Selection Summary                              </span>
<span class="hljs-comment"># ------------------------------------------------------------------------------------</span>
<span class="hljs-comment">#                      Added/                   Adj.                                      </span>
<span class="hljs-comment"># Step    Variable    Removed     R-Square    R-Square     C(p)       AIC        RMSE     </span>
<span class="hljs-comment"># ------------------------------------------------------------------------------------</span>
<span class="hljs-comment">#    1    log(x1)     addition       0.847       0.842    4.9510    153.7173    2.5128    </span>
<span class="hljs-comment">#    2    log(x5)     addition       0.865       0.856    3.0000    151.6296    2.3977    </span>
<span class="hljs-comment"># ------------------------------------------------------------------------------------</span>
</div></code></pre>
<p>Fit the model with selected parameters only.</p>
<pre><code class="language-R"><div>fit5 &lt;- lm(log(y) ~ log(x1) + log(x5), data = data)
summary(fit5)
<span class="hljs-comment">#Output</span>
<span class="hljs-comment"># Residuals:</span>
<span class="hljs-comment">#      Min       1Q   Median       3Q      Max </span>
<span class="hljs-comment"># -0.26474 -0.05966  0.01784  0.07172  0.19876 </span>

<span class="hljs-comment"># Coefficients:</span>
<span class="hljs-comment">#             Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="hljs-comment"># (Intercept)   7.1097     0.4615  15.406 1.68e-15 ***</span>
<span class="hljs-comment"># log(x1)      -0.6324     0.0542 -11.669 1.78e-12 ***</span>
<span class="hljs-comment"># log(x5)      -0.5794     0.1780  -3.255  0.00288 ** </span>
<span class="hljs-comment"># ---</span>
<span class="hljs-comment"># Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span>

<span class="hljs-comment"># Residual standard error: 0.1101 on 29 degrees of freedom</span>
<span class="hljs-comment"># Multiple R-squared:  0.8655,    Adjusted R-squared:  0.8563 </span>
<span class="hljs-comment"># F-statistic: 93.33 on 2 and 29 DF,  p-value: 2.317e-13</span>
plot(fit5, which =<span class="hljs-number">1</span>)
</div></code></pre>
<p>The performance of the model is increased. The Adjusted R-squared raised to 0.8563 and F-statistic is increased to 93.33. The p-value of the predictors is significant. Also the Residual standard error has reduced to 0.1.</p>
<p align="left">
  <img src="file:////Users/mbpro/College/STAT/LRA/finalQ2_fit5_res.png" width="450" title="hover text">
</p>
<p>Outliers Observation:</p>
<p>It is still observered that the are some outliers - point 6, 15, 22, which if eleminated could improve our model. Also from Normal q-q, this model has its residuals normally distributed except for the tail data (the outliers).</p>
<pre><code class="language-R"><div>fit6 &lt;- lm(log(y) ~ log(x1) + log(x5), data = data[ -c(<span class="hljs-number">6</span>, <span class="hljs-number">15</span>, <span class="hljs-number">22</span>), ])
summary(fit6)
<span class="hljs-comment"># Output</span>
<span class="hljs-comment"># Residuals:</span>
<span class="hljs-comment">#      Min       1Q   Median       3Q      Max </span>
<span class="hljs-comment"># -0.15466 -0.03867  0.01436  0.05199  0.12501 </span>

<span class="hljs-comment"># Coefficients:</span>
<span class="hljs-comment">#             Estimate Std. Error t value Pr(&gt;|t|)    </span>
<span class="hljs-comment"># (Intercept)  6.93889    0.36089  19.227  &lt; 2e-16 ***</span>
<span class="hljs-comment"># log(x1)     -0.62182    0.04258 -14.605 4.81e-14 ***</span>
<span class="hljs-comment"># log(x5)     -0.46913    0.13866  -3.383  0.00228 ** </span>
<span class="hljs-comment"># ---</span>
<span class="hljs-comment"># Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span>

<span class="hljs-comment"># Residual standard error: 0.0831 on 26 degrees of freedom</span>
<span class="hljs-comment"># Multiple R-squared:  0.9217,    Adjusted R-squared:  0.9157 </span>
<span class="hljs-comment"># F-statistic: 153.1 on 2 and 26 DF,  p-value: 4.128e-15</span>
</div></code></pre>
<p>Thus in this final model, we have, the performance of the model is again increased. The Adjusted R-squared raised to 0.9157 and F-statistic is increased to 153.1 from 93.33 in previous fit. The p-value of the predictors is significant. Also the Residual standard error has reduced to 0.0831.</p>
<p>ANOVA:</p>
<pre><code class="language-R"><div>anova(fit6)
<span class="hljs-comment">#Output</span>
<span class="hljs-comment"># Analysis of Variance Table</span>

<span class="hljs-comment"># Response: log(y)</span>
<span class="hljs-comment">#           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    </span>
<span class="hljs-comment"># log(x1)    1 2.03568 2.03568 294.798 1.049e-15 ***</span>
<span class="hljs-comment"># log(x5)    1 0.07904 0.07904  11.447  0.002279 ** </span>
<span class="hljs-comment"># Residuals 26 0.17954 0.00691            </span>
</div></code></pre>
<p>The final model is</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mn>6.94</mn><mo>−</mo><mn>0.62</mn><mo>∗</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><mn>0.47</mn><mo>∗</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mn>5</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">log(\hat y) = 6.94 - 0.62 * log(x1) - 0.47 * log(x5)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;">^</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">6</span><span class="mord">.</span><span class="mord">9</span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">6</span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mord">7</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mord">5</span><span class="mclose">)</span></span></span></span></span></p>

    </body>
    </html>